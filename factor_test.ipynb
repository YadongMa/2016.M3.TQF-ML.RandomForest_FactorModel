{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Mar 30 19:05:09 2017\n",
    "\n",
    "@author: Musama\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取原始数据\n",
    "data_dir = 'C:\\\\Users\\\\Musama\\\\Desktop\\\\Factor Models\\\\Data\\\\'\n",
    "ClosePrice =  pd.read_csv(data_dir + 'ClosePrice.csv',header = None).values\n",
    "HighPrice =  pd.read_csv(data_dir + 'HighPrice.csv',header = None).values\n",
    "LowPrice =  pd.read_csv(data_dir + 'LowPrice.csv',header = None).values\n",
    "ListDays =  pd.read_csv(data_dir + 'ListDays.csv',header = None).values\n",
    "SpecialTreat =  pd.read_csv(data_dir + 'SpecialTreat.csv',header = None).values\n",
    "index300 =  pd.read_csv(data_dir + 'index300.csv',header = None).values\n",
    "index500 =  pd.read_csv(data_dir + 'index500.csv',header = None).values\n",
    "\n",
    "# 因子数据\n",
    "\n",
    "# 自变量因子排序后分为五组,因变量分为1，0，1为高收益组\n",
    "labeldata_dir = 'C:\\\\Users\\\\Musama\\\\Desktop\\\\Factor Models\\\\labelData\\\\'\n",
    "Ret_m = pd.read_csv(labeldata_dir + 'Ret_m.csv').values\n",
    "\n",
    "##导入自变量\n",
    "labeldata_dir = 'C:\\\\Users\\\\Musama\\\\Desktop\\\\Factor Models\\\\Data\\\\'\n",
    "TotalValue =  pd.read_csv(labeldata_dir + 'TotalValue.csv',header = None).values\n",
    "PB_lf =  pd.read_csv(labeldata_dir + 'PB_lf.csv',header = None).values\n",
    "HeteroRsquare_FF_d20 =  pd.read_csv(labeldata_dir + 'HeteroRsquare_FF_d20.csv',header = None).values\n",
    "PreRet_M_IndusRet_Citic1_m1 =  pd.read_csv(labeldata_dir + 'PreRet_M_IndusRet_Citic1_m1.csv',header = None).values\n",
    "PreRet_M_IndusRet_Citic1_m3 =  pd.read_csv(labeldata_dir + 'PreRet_M_IndusRet_Citic1_m3.csv',header = None).values\n",
    "Seasonal_NetProfitGrowth_YOY =  pd.read_csv(labeldata_dir + 'Seasonal_NetProfitGrowth_YOY.csv',header = None).values\n",
    "Turnover_ols_TotalValue_d20 =  pd.read_csv(labeldata_dir + 'Turnover_ols_TotalValue_d20.csv',header = None).values\n",
    "Reversal_DongFang =  pd.read_csv(labeldata_dir + 'Reversal_DongFang.csv',header = None).values\n",
    "\n",
    "class Factor_Model():\n",
    "     \n",
    "    def __init__(self,holdperiod,stocknum,feerate,begdate,enddate):\n",
    "        self.holdperiod = holdperiod\n",
    "        self.stocknum = stocknum\n",
    "        self.feerate = feerate\n",
    "        self.begdate = begdate\n",
    "        self.enddate = enddate\n",
    "        self.N_date = 3217        \n",
    "        self.N_stock = 3242\n",
    "\n",
    "    def rank_scorer(self,factor_weight=0.5):\n",
    "        beginidx = 60\n",
    "        SynFactor = np.zeros([self.N_date,self.N_stock])\n",
    "        for t in np.arange(beginidx,self.N_date):\n",
    "            #因子权重，对因子序列加权\n",
    "            factor1 = PB_lf[t,:]\n",
    "            factor2 = PB_lf[t,:]  \n",
    "            # 原数据加权排序打分\n",
    "            tmp_X = factor_weight * factor1 + (1-factor_weight)* factor2\n",
    "            tmpvar1 = np.vstack([np.arange(0,len(tmp_X)),tmp_X]).T\n",
    "            tmpvar1 = pd.DataFrame(tmpvar1,columns = ['id','scores'])\n",
    "            tmpvar1 = tmpvar1.sort_values(by = 'scores')\n",
    "            tmpvar1['rank'] = np.arange(len(tmpvar1.scores))\n",
    "            score1 = tmpvar1.sort_values(by = 'id')['rank']\n",
    "            score1[np.isnan(tmp_X)] = (np.isnan(tmp_X) == False).sum()+1\n",
    "            SynFactor[t,:] = score1\n",
    "        self.SynFactor = SynFactor\n",
    "        return self\n",
    "    \n",
    "            \n",
    "    def scorer_backtest(self):  \n",
    "        # 读取数据\n",
    "        data_dir = 'C:\\\\Users\\\\Musama\\\\Desktop\\\\Factor Models\\\\Data\\\\'\n",
    "        HighPrice =  pd.read_csv(data_dir + 'HighPrice.csv',header = None).values\n",
    "        LowPrice =  pd.read_csv(data_dir + 'LowPrice.csv',header = None).values\n",
    "        ListDays =  pd.read_csv(data_dir + 'ListDays.csv',header = None).values\n",
    "        SpecialTreat =  pd.read_csv(data_dir + 'SpecialTreat.csv',header = None).values\n",
    "        index300 =  pd.read_csv(data_dir + 'index300.csv',header = None)\n",
    "        index500 =  pd.read_csv(data_dir + 'index500.csv',header = None)\n",
    "        DRet = pd.read_csv(data_dir + 'DRet.csv',header = None).values\n",
    "        ret300 = (index300/index300.shift(1)-1).values \n",
    "        ret500 = (index500/index500.shift(1)-1).values \n",
    "        DRet[np.isnan(DRet)] = 0\n",
    "        \n",
    "        begid = 1214 \n",
    "        ret_strategy = np.zeros(self.N_date)\n",
    "        ret_short = np.zeros(self.N_date)\n",
    "        tradetimes = np.ceil((self.N_date-begid)/self.holdperiod)\n",
    "        stock_group = list()\n",
    "        weight_group = list()\n",
    "        predict_score = list()\n",
    "#        SynFactor = self.SynFactor\n",
    "        \n",
    "        #-------------------全市场等权配置----------------\n",
    "        for idx in np.arange(tradetimes,dtype = np.int):\n",
    "            buyid = begid+idx*self.holdperiod+1\n",
    "#            tmpindicator = SynFactor[buyid-1,:]\n",
    "            result = RF_predictor(buyid,method=method)\n",
    "            tmpindicator = -result[0]           #提取预测打分\n",
    "            predict_score.append(result[1])     #存储预测accuracy\n",
    "            invalidindex = np.vstack([HighPrice[buyid,:]==LowPrice[buyid,:] , np.isnan(HighPrice[buyid,:]) , SpecialTreat[buyid-1,:]==1 , ListDays[buyid-1,:]<60,np.isnan(tmpindicator)]).any(axis = 0)        #invalid的股票，向量\n",
    "            valid_idx = invalidindex==False            # 有效的股票\n",
    "            tmpvar2 = np.vstack([np.arange(len(tmpindicator)),tmpindicator]).T\n",
    "            tmpvar2 = tmpvar2[valid_idx,:]\n",
    "            tmpvar2 = pd.DataFrame(tmpvar2,columns = ['id','scores'])\n",
    "            tmpvar2 = tmpvar2.sort_values(by = 'scores')['id'].values     # 提取排序后的股票id            \n",
    "            stock_id = tmpvar2[np.arange(self.stocknum)]\n",
    "            stock_id = stock_id.astype(np.int)                       \n",
    "            short_stock_id = tmpvar2[-self.stocknum:]\n",
    "            short_stock_id = short_stock_id.astype(np.int)                       \n",
    "\n",
    "            # 计算weight\n",
    "            stock_weight = 1/self.stocknum * np.ones(self.stocknum)      # 等权权重，列向量\n",
    "            short_stock_weight = 1/self.stocknum * np.ones(self.stocknum)      # 等权权重，列向量\n",
    "            stock_group.append(stock_id)        # 当期全市场选股id\n",
    "            weight_group.append(stock_weight)          # 当期全市场选股权重     \n",
    "            \n",
    "            # 计算组合收益率\n",
    "            if idx<tradetimes:\n",
    "                tmpret = DRet[(buyid+1):(buyid+self.holdperiod+1),stock_id].dot(stock_weight)      #本次买入后持仓组合收益率\n",
    "                ret_strategy[(buyid+1):(buyid+self.holdperiod+1)] = tmpret\n",
    "                tmpret = DRet[(buyid+1):(buyid+self.holdperiod+1),short_stock_id].dot(short_stock_weight)      #本次买入后持仓组合收益率\n",
    "                ret_short[(buyid+1):(buyid+self.holdperiod+1)] = tmpret\n",
    "            else:\n",
    "                tmpret = DRet[(buyid+1):,stock_id].dot(stock_weight)\n",
    "                ret_strategy[(buyid+1):] = tmpret\n",
    "                tmpret = DRet[(buyid+1):,short_stock_id].dot(short_stock_weight)\n",
    "                ret_short[(buyid+1):] = tmpret\n",
    "            print('Trade Times:%.i completed'%(idx+1))\n",
    "            \n",
    "        ret300[:(begid+2)] = 0\n",
    "        ret_strategy[np.isnan(ret_strategy)] = 0\n",
    "        ret_short[np.isnan(ret_short)] = 0\n",
    "        ret_longshort = ret_strategy - ret_short\n",
    "        ret300[np.isnan(ret500)] = 0                                  \n",
    "        ret_hedge300 = ret_strategy - ret300[:,0]\n",
    "        nav_long = np.cumprod(1+ret_strategy)\n",
    "        nav_longshort = np.cumprod(1+ret_longshort)\n",
    "        nav_hedge300 = np.cumprod(1+ret_hedge300)            \n",
    "        ret500[:(begid+2)] = 0\n",
    "        ret500[np.isnan(ret500)] = 0                                  \n",
    "        ret_hedge500 = ret_strategy - ret500[:,0]\n",
    "        nav_hedge500 = np.cumprod(1+ret_hedge500)\n",
    "        #返回值\n",
    "        self.stcok_group = stock_group\n",
    "        self.weight_group = weight_group\n",
    "        self.pre_score = predict_score\n",
    "        # return\n",
    "        self.ret_strategy = ret_strategy\n",
    "        self.ret_lonngshort= ret_longshort\n",
    "        self.ret_hedge300 = ret_hedge300\n",
    "        self.ret_hedge500 = ret_hedge500\n",
    "        \n",
    "        # nav\n",
    "        self.nav_long = nav_long\n",
    "        self.nav_longshort = nav_longshort\n",
    "        self.nav_hedge300 = nav_hedge300        \n",
    "        self.nav_hedge500 = nav_hedge500\n",
    "        \n",
    "        # annulised return\n",
    "        self.return_long = self.nav_long[-1]**(245/(self.N_date-begid))-1\n",
    "        self.return_longshort = self.nav_longshort[-1]**(245/(self.N_date-begid))-1\n",
    "        self.return_hedge300 = self.nav_hedge300[-1]**(245/(self.N_date-begid))-1\n",
    "        self.volatility_hedge300 = np.std(self.ret_hedge300)*np.sqrt(245)        \n",
    "        self.IR_hedge300 = self.return_hedge300/self.volatility_hedge300\n",
    "        \n",
    "        self.return_hedge500 = self.nav_hedge500[-1]**(245/(self.N_date-begid))-1\n",
    "        self.volatility_hedge500 = np.std(self.ret_hedge500)*np.sqrt(245)        \n",
    "        self.IR_hedge500 = self.return_hedge500/self.volatility_hedge500\n",
    "        return self      \n",
    "        \n",
    "    def performance_report(self):\n",
    "        \n",
    "        # drawdown\n",
    "        drawdown_long = [self.nav_long[i]/np.max(self.nav_long[:(i+1)])-1 for i in np.arange(self.N_date)]\n",
    "        drawdown_longshort = [self.nav_longshort[i]/np.max(self.nav_longshort[:(i+1)])-1 for i in np.arange(self.N_date)]\n",
    "        max_drawdownlongshort_end = np.where(drawdown_longshort==np.min(drawdown_longshort))[0][0]\n",
    "        max_drawdownlongshort_beg = np.where(self.nav_hedge300[:(max_drawdownlongshort_end+1)]==np.max(self.nav_hedge300[:(max_drawdownlongshort_end+1)]))[0][0]\n",
    "\n",
    "        # hedge 300\n",
    "        drawdown_hedge300 = [self.nav_hedge300[i]/np.max(self.nav_hedge300[:(i+1)])-1 for i in np.arange(self.N_date)]                \n",
    "        max_drawdown300_end = np.where(drawdown_hedge300==np.min(drawdown_hedge300))[0][0]\n",
    "        max_drawdown300_beg = np.where(self.nav_hedge300[:(max_drawdown300_end+1)]==np.max(self.nav_hedge300[:(max_drawdown300_end+1)]))[0][0]\n",
    "\n",
    "        # hedge 300                 \n",
    "        drawdown_hedge500 = [self.nav_hedge500[i]/np.max(self.nav_hedge500[:(i+1)])-1 for i in np.arange(self.N_date)]                \n",
    "        max_drawdown500_end = np.where(drawdown_hedge500==np.min(drawdown_hedge500))[0][0]\n",
    "        max_drawdown500_beg = np.where(self.nav_hedge500[:(max_drawdown500_end+1)]==np.max(self.nav_hedge500[:(max_drawdown500_end+1)]))[0][0]\n",
    "        \n",
    "        self.drawdown_long = drawdown_long\n",
    "        self.drawdown_longshort = drawdown_longshort\n",
    "        self.max_drawdown_longshort= np.min(drawdown_longshort)\n",
    "        self.max_drawdown_longshort_period = [max_drawdownlongshort_beg,max_drawdownlongshort_end]\n",
    "        \n",
    "        self.draw_hedge300 = drawdown_hedge300\n",
    "        self.max_drawdown_hedge300 = np.min(drawdown_hedge300)\n",
    "        self.max_drawdown_hedge300_period = [max_drawdown300_beg,max_drawdown300_end]\n",
    "\n",
    "        self.draw_hedge500 = drawdown_hedge500\n",
    "        self.max_drawdown_hedge500 = np.min(drawdown_hedge500)\n",
    "        self.max_drawdown_hedge500_period = [max_drawdown500_beg,max_drawdown500_end]\n",
    "        df = pd.DataFrame([self.nav_long,self.nav_longshort,self.nav_hedge300,self.nav_hedge500]).T\n",
    "        df.columns = ['Long Postition','Longshort Hedging','Hedging HS300','Hedging ZZ500']\n",
    "        df.plot(figsize = [12,8],grid=1)\n",
    "        print('-------------------------------------------------------------------------')\n",
    "        print('Long position annualized return: %.4f'%(self.return_long))\n",
    "        print('Long Short annualized return: %.4f'%(self.return_longshort))\n",
    "        print('Long Short Maxmium Drawdown: %.3f, Drawdown period: from %i, to %i'\\\n",
    "              %(self.max_drawdown_longshort,max_drawdownlongshort_beg,max_drawdownlongshort_end))\n",
    "        print('Hedging HS300 annualized return: %.4f'%(self.return_hedge300))\n",
    "        print('Hedging HS300 Maxmium Drawdown: %.3f, Drawdown period: from %i, to %i'\\\n",
    "              %(self.max_drawdown_hedge300,max_drawdown300_beg,max_drawdown300_end))\n",
    "        print('Hedging HS500 annualized return: %.4f'%(self.return_hedge500))\n",
    "        print('Hedging HS500 Maxmium Drawdown: %.3f, Drawdown period: from %i, to %i'\\\n",
    "              %(self.max_drawdown_hedge500,max_drawdown500_beg,max_drawdown500_end))\n",
    "        print('-------------------------------------------------------------------------')\n",
    "        return self\n",
    "\n",
    "        \n",
    "    \n",
    "def RF_predictor(buyid,method,period = 12, freq=20):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    stdsc = StandardScaler()    \n",
    "    # 构造自变量\n",
    "    idx = np.linspace(buyid-21-freq*period,buyid-21,period,dtype = np.int)    \n",
    "    # 预测期输入变量\n",
    "    factor1_pre = TotalValue[buyid-1,:]\n",
    "    factor2_pre = PB_lf[buyid-1,:]\n",
    "    factor3_pre = Reversal_DongFang[buyid-1,:]\n",
    "    factor4_pre = HeteroRsquare_FF_d20[buyid-1,:]   \n",
    "    factor5_pre = PreRet_M_IndusRet_Citic1_m1[buyid-1,:]    \n",
    "    factor6_pre = PreRet_M_IndusRet_Citic1_m3 [buyid-1,:]\n",
    "    factor7_pre = Seasonal_NetProfitGrowth_YOY[buyid-1,:]\n",
    "    factor8_pre = Turnover_ols_TotalValue_d20[buyid-1,:]\n",
    "\n",
    "\n",
    "    # 并为预测自变量矩阵\n",
    "    X_pre = np.vstack([factor1_pre,factor2_pre,factor3_pre,factor4_pre,\\\n",
    "                       factor5_pre,factor6_pre,factor7_pre,factor8_pre]).T\n",
    "\n",
    "        \n",
    "    # 剔除停牌、ST\n",
    "    invalidindex = np.vstack([np.isnan(X_pre).any(axis=1),HighPrice[buyid,:]==LowPrice[buyid,:] , np.isnan(HighPrice[buyid,:]) , SpecialTreat[buyid-1,:]==1 , ListDays[buyid-1,:]<60]).any(axis = 0)        #invalid的股票，向量\n",
    "    valid_idx = invalidindex ==False        #有效的id\n",
    "    X_pre = X_pre[valid_idx,:]          #有效的预测自变量\n",
    "    X_pre = stdsc.fit_transform(X_pre)\n",
    "    y_true = Ret_m[buyid-1,:][valid_idx]\n",
    "    # 训练组自变量\n",
    "    factor1 = np.concatenate(np.log(TotalValue[idx,:]))\n",
    "    factor2 = np.concatenate(PB_lf[idx,:])\n",
    "    factor3 = np.concatenate(Reversal_DongFang[idx,:])\n",
    "    factor4 = np.concatenate(HeteroRsquare_FF_d20[idx,:])   \n",
    "    factor5 = np.concatenate(PreRet_M_IndusRet_Citic1_m1[idx,:])    \n",
    "    factor6 = np.concatenate(PreRet_M_IndusRet_Citic1_m3[idx,:])\n",
    "    factor7 = np.concatenate(Seasonal_NetProfitGrowth_YOY[idx,:])\n",
    "    factor8 = np.concatenate(Turnover_ols_TotalValue_d20[idx,:])\n",
    "\n",
    "    X = np.vstack([factor1,factor2,factor3,factor4,factor5,\\\n",
    "                       factor6,factor7,factor8]).T\n",
    "                   \n",
    "    y = np.concatenate(Ret_m[idx,:])\n",
    "    df = pd.DataFrame(np.column_stack([X,y]))\n",
    "    df = df.dropna()\n",
    "    X_train = df.iloc[:,:-1].values    \n",
    "    X_train = stdsc.fit_transform(X_train)\n",
    "    y_train = df.iloc[:,-1].values\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    if method=='LinearRegression':\n",
    "        #Linear Regression回归\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        ols = LinearRegression()\n",
    "        ols = ols.fit(X_train,y_train)\n",
    "        ret_pre = ols.predict(X_pre)\n",
    "        y_pre = np.where(ret_pre>0.5,1,0)\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])\n",
    "    elif method=='LogisticRegression':\n",
    "        #Logistic分类器\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr = LogisticRegression()\n",
    "        lr = lr.fit(X_train,y_train)\n",
    "        y_pre = lr.predict(X_pre)\n",
    "        ret_pre = lr.predict_proba(X_pre)[:,1]\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])\n",
    "    elif method=='RandomForest':\n",
    "        #Random Forest分类器\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        rf = RandomForestClassifier(n_estimators=300,max_features='auto',n_jobs=-1)\n",
    "        rf = rf.fit(X_train,y_train)\n",
    "        y_pre = rf.predict(X_pre)\n",
    "        ret_pre = rf.predict_proba(X_pre)[:,1]\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])\n",
    "    elif method=='RandomForest_Regression':\n",
    "        #Random Forest分类器\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        rf = RandomForestRegressor(n_estimators=300,max_features='auto',n_jobs=-1)\n",
    "        rf = rf.fit(X_train,y_train)\n",
    "        ret_pre = rf.predict(X_pre)\n",
    "        y_pre = np.where(ret_pre>0.5,1,0)\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])\n",
    "    elif method=='Adaboost':\n",
    "        #AdaBoost分类器    \n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        adb = AdaBoostClassifier(n_estimators=300)\n",
    "        adb = adb.fit(X_train,y_train)\n",
    "        y_pre = adb.predict(X_pre)\n",
    "        ret_pre = adb.predict_proba(X_pre)[:,1]\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])\n",
    "    elif method=='Adaboost_Regression':\n",
    "        #AdaBoost分类器    \n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        adb = AdaBoostRegressor(n_estimators=300)\n",
    "        adb = adb.fit(X_train,y_train)\n",
    "        ret_pre = adb.predict(X_pre)\n",
    "        y_pre = np.where(ret_pre>0.5,1,0)\n",
    "        tmp = pd.DataFrame([y_true,y_pre]).T.dropna().values \n",
    "        score = accuracy_score(tmp[:,0],tmp[:,1])  \n",
    "        \n",
    "        \n",
    "    result = np.zeros(len(valid_idx))\n",
    "    result[:] =np.nan\n",
    "    result[valid_idx] = ret_pre\n",
    "    return result,score\n",
    "\n",
    "\n",
    "    \n",
    "#单因子\n",
    "#------------------------------------------------------------------------------\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 100,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.rank_scorer()\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('PB_lf.csv')          \n",
    "\n",
    "        \n",
    "#------------------------------------------------------------------------------    \n",
    "method = 'LinearRegression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'LinearRegression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "method = 'LogisticRegression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'LogisticRegression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 100,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_100.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 300,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_300.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'Adaboost'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "method = 'Adaboost'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'RandomForest_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 300,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_300.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'Adaboost_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 150,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_150.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'Adaboost_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 200,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_200.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')\n",
    "\n",
    "\n",
    "method = 'Adaboost_Regression'\n",
    "f1 = Factor_Model(holdperiod = 20,stocknum = 300,feerate = 0,begdate = '2009/01/01',enddate ='2017/01/01')\n",
    "f1.scorer_backtest()\n",
    "f1.performance_report()\n",
    "ret = pd.DataFrame([f1.ret_lonngshort,f1.ret_hedge300,f1.ret_hedge500]).T     \n",
    "ret.to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_300.csv')                                                                                                  \n",
    "pd.Series(f1.pre_score).to_csv('C:\\\\Users\\\\Musama\\\\Desktop\\\\'+method+'_score.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
